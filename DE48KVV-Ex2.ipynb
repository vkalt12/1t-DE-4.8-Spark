{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 2\n",
    "\n",
    "    2.1 Условие\n",
    "Вам доступны следующие фреймы данных:\n",
    "А)Таблица с информацией о среднедневном спросе на каждый товар для каждой локации:\n",
    "\n",
    "product\tlocation\tdemand\n",
    "1\t01\t100\n",
    "1\t02\t110\n",
    "2\t01\t120\n",
    "2\t02\t90\n",
    "3\t01\t70\n",
    "3\t02\t80\n",
    "\n",
    "product — уникальный идентификатор продукта\n",
    "\n",
    "location — уникальный идентификатор локации\n",
    "\n",
    "demand — значение среднедневного спроса на конкретный товар в конкретной локации в единицах товара\n",
    "\n",
    "Б)Таблица с информацией о складских запасах на уровне месяца:\n",
    "\n",
    "product\tlocation\tstock\n",
    "1\t01\t1000\n",
    "1\t02\t400\n",
    "2\t01\t300\n",
    "2\t02\t250\n",
    "\n",
    "product — уникальный идентификатор продукта\n",
    "stock — уровень запасов в единицах товара на уровне месяца\n",
    "\n",
    "\n",
    "    2.2 Задание\n",
    "\n",
    "В качестве примера рассмотрим июнь 2023 года. \n",
    "Вам необходимо сформировать витрину данных, в которой для каждой пары товар-локация на уровне каждой технической недели* будет рассчитано прогнозируемое значение количества проданных товаров (с учетом среднедневного спроса) и количество остатков товара на складе. \n",
    "\n",
    "Примечание:\n",
    "*Техническая неделя — это календарная неделя или часть календарной недели, которая «укладывается» в один календарный месяц. \n",
    "Например, в августе 2023 года вам доступны следующие технические недели: \n",
    "01.08—06.08\n",
    "07.08—13.08\n",
    "14.08—20.08\n",
    "21.08—27.08\n",
    "28.08—31.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортирование необходимых библиотек pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/17 14:52:40 WARN Utils: Your hostname, alto-pc resolves to a loopback address: 127.0.1.1; using 192.168.181.138 instead (on interface eth0)\n",
      "24/08/17 14:52:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/17 14:52:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/17 14:52:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Инициализация Spark-сессии\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"onlineretail\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productA: long (nullable = true)\n",
      " |-- locationA: string (nullable = true)\n",
      " |-- demand: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|productA|locationA|demand|\n",
      "+--------+---------+------+\n",
      "|1       |01       |100   |\n",
      "|1       |02       |110   |\n",
      "|2       |01       |120   |\n",
      "|2       |02       |90    |\n",
      "|3       |01       |70    |\n",
      "|3       |02       |80    |\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Таблица с информацией о среднедневном спросе на каждый товар для каждой локации\n",
    "# Создание DataFrame\n",
    "\n",
    "data2a = ((1, \"01\", 100), \\\n",
    "    (1, \"02\", 110), \\\n",
    "    (2, \"01\", 120), \\\n",
    "    (2, \"02\", 90), \\\n",
    "    (3, \"01\", 70), \\\n",
    "    (3, \"02\", 80), \\\n",
    "  )\n",
    "\n",
    "columns2a = [\"productA\", \"locationA\", \"demand\"]\n",
    "\n",
    "sparkDF2a = spark.createDataFrame(data = data2a, schema = columns2a)\n",
    "\n",
    "sparkDF2a.printSchema()\n",
    "sparkDF2a.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productB: long (nullable = true)\n",
      " |-- locationB: string (nullable = true)\n",
      " |-- stock: long (nullable = true)\n",
      "\n",
      "+--------+---------+-----+\n",
      "|productB|locationB|stock|\n",
      "+--------+---------+-----+\n",
      "|1       |01       |1000 |\n",
      "|1       |02       |400  |\n",
      "|2       |01       |300  |\n",
      "|2       |02       |250  |\n",
      "+--------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Таблица с информацией о складских запасах на уровне месяца:\n",
    "# Создание DataFrame\n",
    "\n",
    "data2b = ((1, \"01\", 1000), \\\n",
    "    (1, \"02\", 400), \\\n",
    "    (2, \"01\", 300), \\\n",
    "    (2, \"02\", 250), \\\n",
    "  )\n",
    "\n",
    "columns2b = [\"productB\", \"locationB\", \"stock\"]\n",
    "\n",
    "sparkDF2b = spark.createDataFrame(data = data2b, schema = columns2b)\n",
    "\n",
    "sparkDF2b.printSchema()\n",
    "sparkDF2b.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо сформировать витрину данных, в которой для каждой пары товар-локация на уровне каждой технической недели* будет рассчитано прогнозируемое значение количества проданных товаров (с учетом среднедневного спроса) и количество остатков товара на складе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Таблица с информацией о технических неделях июня 2023 года:\n",
    "# Создание DataFrame\n",
    "\n",
    "t_weeks = ((1, 1, 4, 4), \\\n",
    "    (2, 5, 11, 7), \\\n",
    "    (3, 12, 18, 7), \\\n",
    "    (4, 19, 25, 7), \\\n",
    "    (5, 26, 30, 5), \\\n",
    "  )\n",
    "columns2b = [\"week_number\", \"start_day\", \"end_day\", \"days_count\"]\n",
    "spark_t_weeks = spark.createDataFrame(data = t_weeks, schema = columns2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-----+\n",
      "|product|location|demand|stock|\n",
      "+-------+--------+------+-----+\n",
      "|      1|      01|   100| 1000|\n",
      "|      1|      02|   110|  400|\n",
      "|      2|      01|   120|  300|\n",
      "|      2|      02|    90|  250|\n",
      "|      3|      01|    70|    0|\n",
      "|      3|      02|    80|    0|\n",
      "+-------+--------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showcase = sparkDF2a.join(sparkDF2b\\\n",
    "               , on = ((sparkDF2a.productA == sparkDF2b.productB) \\\n",
    "               & (sparkDF2a.locationA == sparkDF2b.locationB))\\\n",
    "                , how = \"full\").select(F.col(\"productA\").alias(\"product\"),\\\n",
    "                                       F.col(\"locationA\").alias(\"location\"),\\\n",
    "                                       F.col(\"demand\"),\n",
    "                                       F.col(\"stock\"))\n",
    "showcase = showcase.na.fill(0)                                        \n",
    "showcase.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-----+----------+-----------+----------+-----------+----------+-----------+----------+-----------+----------+-----------+\n",
      "|product|location|demand|stock|sold_week1|stock_week1|sold_week2|stock_week2|sold_week3|stock_week3|sold_week4|stock_week4|sold_week5|stock_week5|\n",
      "+-------+--------+------+-----+----------+-----------+----------+-----------+----------+-----------+----------+-----------+----------+-----------+\n",
      "|      1|      01|   100| 1000|       400|        600|       600|          0|         0|          0|         0|          0|         0|          0|\n",
      "|      1|      02|   110|  400|       400|          0|         0|          0|         0|          0|         0|          0|         0|          0|\n",
      "|      2|      01|   120|  300|       300|          0|         0|          0|         0|          0|         0|          0|         0|          0|\n",
      "|      2|      02|    90|  250|       250|          0|         0|          0|         0|          0|         0|          0|         0|          0|\n",
      "|      3|      01|    70|    0|         0|          0|         0|          0|         0|          0|         0|          0|         0|          0|\n",
      "|      3|      02|    80|    0|         0|          0|         0|          0|         0|          0|         0|          0|         0|          0|\n",
      "+-------+--------+------+-----+----------+-----------+----------+-----------+----------+-----------+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/17 14:52:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Витрина данных, в которой для каждой пары товар-локация \n",
    "# на уровне каждой технической недели рассчитано \n",
    "# прогнозируемое значение количества проданных товаров (sold_week_)\n",
    "# (с учетом среднедневного спроса) \n",
    "# и количество остатков товара на складе (stock_week_).\n",
    "\n",
    "showcase_weeks = showcase\\\n",
    "    .withColumn(\"sold_week1\", F.when(F.lit(F.col(\"demand\")*4) < F.col(\"stock\"),\\\n",
    "        F.lit(F.col(\"demand\")*4))\\\n",
    "        .otherwise(F.col(\"stock\")))\\\n",
    "    .withColumn(\"stock_week1\",\\\n",
    "        F.when((F.lit(F.col(\"stock\") - F.col(\"demand\")*4)) > 0,\\\n",
    "        F.lit(F.col(\"stock\") - F.col(\"demand\")*4))\\\n",
    "        .otherwise(0))\\\n",
    "    .withColumn(\"sold_week2\", F.when(F.lit(F.col(\"demand\")*7) < F.col(\"stock_week1\"),\\\n",
    "        F.lit(F.col(\"demand\")*7))\\\n",
    "        .otherwise(F.col(\"stock_week1\")))\\\n",
    "    .withColumn(\"stock_week2\", \\\n",
    "        F.when((F.lit(F.col(\"stock_week1\") - F.col(\"demand\")*(7))) > 0,\\\n",
    "        F.lit(F.col(\"stock_week1\") - F.col(\"demand\")*(7)))\\\n",
    "        .otherwise(0))\\\n",
    "    .withColumn(\"sold_week3\", F.when(F.lit(F.col(\"demand\")*7) < F.col(\"stock_week2\"),\\\n",
    "        F.lit(F.col(\"demand\")*7))\\\n",
    "        .otherwise(F.col(\"stock_week2\")))\\\n",
    "    .withColumn(\"stock_week3\", \\\n",
    "        F.when((F.lit(F.col(\"stock\") - F.col(\"demand\")*(4+7+7))) > 0,\\\n",
    "        F.lit(F.col(\"stock\") - F.col(\"demand\")*(4+7+7)))\\\n",
    "        .otherwise(0))\\\n",
    "    .withColumn(\"sold_week4\", F.when(F.lit(F.col(\"demand\")*7) < F.col(\"stock_week3\"),\\\n",
    "        F.lit(F.col(\"demand\")*7))\\\n",
    "        .otherwise(F.col(\"stock_week3\")))\\\n",
    "    .withColumn(\"stock_week4\", \\\n",
    "        F.when((F.lit(F.col(\"stock\") - F.col(\"demand\")*(4+7+7+7))) > 0,\\\n",
    "        F.lit(F.col(\"stock\") - F.col(\"demand\")*(4+7+7+7)))\\\n",
    "        .otherwise(0))\\\n",
    "    .withColumn(\"sold_week5\", F.when(F.lit(F.col(\"demand\")*5) < F.col(\"stock_week4\"),\\\n",
    "        F.lit(F.col(\"demand\")*5))\\\n",
    "        .otherwise(F.col(\"stock_week4\")))\\\n",
    "    .withColumn(\"stock_week5\", \\\n",
    "        F.when((F.lit(F.col(\"stock\") - F.col(\"demand\")*(4+7+7+7+5))) > 0,\\\n",
    "        F.lit(F.col(\"stock\") - F.col(\"demand\")*(4+7+7+7+5)))\\\n",
    "        .otherwise(0))\n",
    "\n",
    "showcase_weeks.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
